{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3a72de",
   "metadata": {},
   "source": [
    "# Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45987661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "import nltk\n",
    "import numpy\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274f7b3",
   "metadata": {},
   "source": [
    "# Importation data frames et nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFnames = [\"cheyenne\", \"newYork\", \"newportBay\", \"sequoiaLodge\", \"santaFe\", \"davyCrockettRanch\"]\n",
    "\n",
    "#Importation from csv\n",
    "disney = pd.DataFrame(columns = [\"Prenom\",\"Note\",\"Pays\",\"Titre\",\"Positif\",\"Négatif\",\"Date séjour\",\"Date commentaire\",\"hotel\"])\n",
    "for i in range(len(DFnames)): \n",
    "    nameDF = DFnames[i]\n",
    "    DF = pd.read_csv(\"./data/\"+nameDF+\".csv\", index_col=0)  \n",
    "    DF[\"hotel\"] = nameDF\n",
    "    disney = pd.concat([disney, DF])\n",
    "    globals()[nameDF] = DF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyses\n",
    "from nettoyage import nettoyage_corpus\n",
    "\n",
    "#Filtre france en attendant\n",
    "disney = disney[disney.Pays == \"France\"]\n",
    "corpusPos = disney.Positif.tolist()\n",
    "\n",
    "#nettoyage des nan\n",
    "corpusPos = [str(i) for i in corpusPos]\n",
    "\n",
    "#Nettoyage global du corpus\n",
    "corpusPos = nettoyage_corpus(corpusPos)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "modele = Word2Vec(corpusPos,vector_size=2,window=3,min_count=1)\n",
    "\n",
    "#propriété \"wv\" -> wordvector\n",
    "words = modele.wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ebc6e",
   "metadata": {},
   "source": [
    "# Analyse Exploratoire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288df793",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(modele)\n",
    "\n",
    "#dimensionnalité\n",
    "print(modele.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111443cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taille du voisinage\n",
    "modele.window\n",
    "\n",
    "#dimension de la représentation\n",
    "words.vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage des termes de leur index\n",
    "words.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b2443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taille du dictionnaire\n",
    "len(words.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9cbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les clés : les termes\n",
    "words.key_to_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarité entre prix et cher\n",
    "words.similarity(\"prix\",\"cher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312830a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les termes les plus proches de \"prix\"\n",
    "words.most_similar(\"prix\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les termes les plus proches de \"problème\"\n",
    "words.most_similar(\"problème\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.most_similar(\"personnel\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = ['séjour','personnel','prix','bien','super','cher']\n",
    "words.doesnt_match(liste)\n",
    "\n",
    "#data frame des coordonnées\n",
    "import pandas\n",
    "df = pandas.DataFrame(words.vectors,columns=['V1','V2'],index=words.key_to_index.keys())\n",
    "print(df)\n",
    "\n",
    "#sous-data frame corresp. aux termes à étudier\n",
    "dfListe = df.loc[liste,:]\n",
    "dfListe\n",
    "\n",
    "#graphique dans le plan\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(dfListe.V1,dfListe.V2,s=0.5)\n",
    "for i in range(dfListe.shape[0]):\n",
    "    plt.annotate(dfListe.index[i],(dfListe.V1[i],dfListe.V2[i]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4bd89b",
   "metadata": {},
   "source": [
    "# Clusters des mots les plus proches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour transformer un document en vecteur\n",
    "#à partir des tokens qui le composent\n",
    "#entrée : doc à traiter\n",
    "#         modèle entrainé ou préentrainé\n",
    "#sortie : vecteur représentant le document\n",
    "def my_doc_2_vec(doc,trained):\n",
    "    #dimension de représentation\n",
    "    p = trained.vectors.shape[1]\n",
    "    #initialiser le vecteur\n",
    "    vec = numpy.zeros(p)\n",
    "    #nombre de tokens trouvés\n",
    "    nb = 0\n",
    "    #traitement de chaque token du document\n",
    "    for tk in doc:\n",
    "        #ne traiter que les tokens reconnus\n",
    "        if ((tk in trained.key_to_index.keys()) == True):\n",
    "            values = trained[tk]\n",
    "            vec = vec + values\n",
    "            nb = nb + 1.0\n",
    "    #faire la moyenne des valeurs\n",
    "    #uniquement si on a trouvé des tokens reconnus bien sûr\n",
    "    if (nb > 0.0):\n",
    "        vec = vec/nb\n",
    "    #renvoyer le vecteur\n",
    "    #si aucun token trouvé, on a un vecteur de valeurs nulles\n",
    "    return vec\n",
    "\n",
    "\n",
    "#fonction pour représenter un corpus à partir d'une représentation\n",
    "#soit entraînée, soit pré-entraînée\n",
    "#sortie : représentation matricielle\n",
    "def my_corpora_2_vec(corpora,trained):\n",
    "    docsVec = list()\n",
    "    #pour chaque document du corpus nettoyé\n",
    "    for doc in corpora:\n",
    "        #calcul de son vecteur\n",
    "        vec = my_doc_2_vec(doc,trained)\n",
    "        #ajouter dans la liste\n",
    "        docsVec.append(vec)\n",
    "    #transformer en matrice numpy\n",
    "    matVec = numpy.array(docsVec)\n",
    "    return matVec\n",
    "\n",
    "#CAH à partir de scipy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage,fcluster\n",
    "\n",
    "#pour transformation en MDT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#fonction pour construire une typologie à partir\n",
    "#d'une représentation des termes, qu'elle soit entraînée ou pré-entraînée\n",
    "#seuil par défaut = 1, mais le but est d'avoir 4 groupes\n",
    "#corpus ici se présente sous la forme d'une liste de listes de tokens\n",
    "def my_cah_from_doc2vec(corpus,trained,seuil=1.0,nbTermes=7):\n",
    "\n",
    "    #matrice doc2vec pour la représentation à 100 dim.\n",
    "    #entraînée via word2vec sur les documents du corpus\n",
    "    mat = my_corpora_2_vec(corpus,trained)\n",
    "\n",
    "    #dimension\n",
    "    #mat.shape\n",
    "\n",
    "    #générer la matrice des liens\n",
    "    Z = linkage(mat,method='ward',metric='euclidean')\n",
    "\n",
    "    #affichage du dendrogramme\n",
    "    plt.title(\"CAH\")\n",
    "    dendrogram(Z,orientation='left',color_threshold=0)\n",
    "    plt.show()\n",
    "\n",
    "    #affichage du dendrogramme avec le seuil\n",
    "    plt.title(\"CAH\")\n",
    "    dendrogram(Z,orientation='left',color_threshold=seuil)\n",
    "    plt.show()\n",
    "\n",
    "    #découpage en 4 classes\n",
    "    grCAH = fcluster(Z,t=seuil,criterion='distance')\n",
    "    #print(grCAH)\n",
    "\n",
    "    #comptage\n",
    "    print(numpy.unique(grCAH,return_counts=True))\n",
    "\n",
    "    #***************************\n",
    "    #interprétation des clusters\n",
    "    #***************************\n",
    "    \n",
    "    #parseur\n",
    "    parseur = CountVectorizer(binary=True)\n",
    "    \n",
    "    #former corpus sous forme de liste de chaîne\n",
    "    corpus_string = [\" \".join(doc) for doc in corpus]\n",
    "    \n",
    "    #matrice MDT\n",
    "    mdt = parseur.fit_transform(corpus_string).toarray()\n",
    "    print(\"Dim. matrice documents-termes = {}\".format(mdt.shape))\n",
    "    \n",
    "    #passer en revue les groupes\n",
    "    for num_cluster in range(numpy.max(grCAH)):\n",
    "        print(\"\")\n",
    "        #numéro du cluster à traiter\n",
    "        print(\"Numero du cluster = {}\".format(num_cluster+1))\n",
    "        groupe = numpy.where(grCAH==num_cluster+1,1,0)\n",
    "        effectifs = numpy.unique(groupe,return_counts=True)\n",
    "        print(\"Effectifs = {}\".format(effectifs[1][1]))\n",
    "        #calcul de co-occurence\n",
    "        cooc = numpy.apply_along_axis(func1d=lambda x: numpy.sum(x*groupe),axis=0,arr=mdt)\n",
    "        #print(cooc)\n",
    "        #création d'un data frame intermédiaire\n",
    "        tmpDF = pandas.DataFrame(data=cooc,columns=['freq'],index=parseur.get_feature_names_out())    \n",
    "        #affichage des \"nbTermes\" termes les plus fréquents\n",
    "        print(tmpDF.sort_values(by='freq',ascending=False).iloc[:nbTermes,:])\n",
    "        \n",
    "    #renvoyer l'indicateur d'appartenance aux groupes\n",
    "    return grCAH, mat\n",
    "\n",
    "#*** fin de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e16ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruire la représentation ci-dessus, mais à 100 dim.\n",
    "modeleBis = Word2Vec(corpusPos,vector_size=100,window=3,min_count=1,epochs=100)\n",
    "wordsBis = modeleBis.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51128563",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1,mat1 = my_cah_from_doc2vec(corpusPos,wordsBis,seuil=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634a3ec",
   "metadata": {},
   "source": [
    "# Analyse des topics les plus associés entre eux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "#dictionnaire de mots avec fréquence d'apparition\n",
    "dictionary = corpora.Dictionary(corpusPos)\n",
    "\n",
    "# enlever les mots peu utilisés\n",
    "dictionary.filter_extremes(no_below= 30, keep_n=1000)\n",
    "\n",
    "#création du Corpus\n",
    "corpusD = [dictionary.doc2bow(text) for text in corpusPos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpusD, num_topics = 3, id2word=dictionary, passes = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954198cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words = 4)\n",
    "print(topics)\n",
    "\n",
    "#Ajouter des stopwords pour améliorer ? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
